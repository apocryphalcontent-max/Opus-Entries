"""
LLM client for interacting with local language models
"""
import json
import requests
from typing import Dict, Any, Optional


class LLMClient:
    """Client for interacting with local LLM models (Ollama-compatible)"""
    
    def __init__(self, base_url: str = "http://localhost:11434", timeout: int = 300):
        """
        Initialize LLM client
        
        Args:
            base_url: Base URL for the LLM API
            timeout: Request timeout in seconds
        """
        self.base_url = base_url.rstrip('/')
        self.timeout = timeout
    
    def generate(
        self, 
        prompt: str, 
        model: str = "llama2",
        system_prompt: Optional[str] = None,
        **kwargs
    ) -> str:
        """
        Generate text using the LLM
        
        Args:
            prompt: The prompt to send to the model
            model: The model to use
            system_prompt: Optional system prompt
            **kwargs: Additional parameters for the API
        
        Returns:
            Generated text
        """
        url = f"{self.base_url}/api/generate"
        
        payload = {
            "model": model,
            "prompt": prompt,
            "stream": False,
            **kwargs
        }
        
        if system_prompt:
            payload["system"] = system_prompt
        
        try:
            response = requests.post(url, json=payload, timeout=self.timeout)
            response.raise_for_status()
            
            result = response.json()
            return result.get("response", "")
        
        except requests.exceptions.RequestException as e:
            # If API is not available, return a placeholder for development
            return self._generate_fallback(prompt, model)
    
    def _generate_fallback(self, prompt: str, model: str) -> str:
        """
        Fallback text generation when LLM is not available
        This is a placeholder for development/testing
        """
        return f"[Generated content for: {prompt[:100]}...]\n\nThis is placeholder content generated because the LLM service is not available. In a production environment, this would be replaced with actual content generated by the {model} model.\n\nThe content would explore the topic from an Orthodox Christian perspective, integrating theological, philosophical, mathematical, and scientific insights while maintaining fidelity to Patristic tradition and contemporary Orthodox thought."
    
    def check_connection(self) -> bool:
        """
        Check if the LLM service is available
        
        Returns:
            True if service is available, False otherwise
        """
        try:
            response = requests.get(f"{self.base_url}/api/tags", timeout=5)
            return response.status_code == 200
        except requests.exceptions.RequestException:
            return False
